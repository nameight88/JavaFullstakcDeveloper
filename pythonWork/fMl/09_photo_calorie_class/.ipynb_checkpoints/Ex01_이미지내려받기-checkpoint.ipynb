{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 다운로드는 시간 소요됨\n",
    "\n",
    "미리 다운 받은 사진 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flickrapi\n",
      "  Downloading flickrapi-2.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from flickrapi) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.2.1 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from flickrapi) (2.31.0)\n",
      "Collecting requests-oauthlib>=0.4.0 (from flickrapi)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=0.3.1 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from flickrapi) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from requests>=2.2.1->flickrapi) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from requests>=2.2.1->flickrapi) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from requests>=2.2.1->flickrapi) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ict03_013\\anaconda3\\lib\\site-packages (from requests>=2.2.1->flickrapi) (2024.2.2)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.4.0->flickrapi)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading flickrapi-2.4.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: oauthlib, requests-oauthlib, flickrapi\n",
      "Successfully installed flickrapi-2.4.0 oauthlib-3.2.2 requests-oauthlib-2.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install flickrapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flickr로 사진 검색해서 다운로드하기\n",
    "from flickrapi import FlickrAPI\n",
    "from urllib.request import urlretrieve\n",
    "from pprint import pprint\n",
    "import os, time, sys\n",
    "\n",
    "# AP 키 지정하기--- ( ※ 1)\n",
    "# key = \"<자신의 것을 입력해주세요>\"\n",
    "# secret = \"<자신의 것을 입력해주세요>\"\n",
    "key = \"1eb22788a01f7bace9649c51c3de8c03\"\n",
    "secret = \"87b0bdb8d6b17cc5\"\n",
    "wait_time = 1 # 대기 시간(초)\n",
    "\n",
    "# 한 번에 대량의 이미지를 내려 받으면 Flickr 서버에 부하가 걸릴 수 있기에 1초 간격으로 내려받고자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "# Flickr API로 사진 검색하기 \n",
    "def go_download(keyword, dir):\n",
    "    # 저장 경로 지정하기\n",
    "    savedir = \"./image/\" + dir\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    # API를 사용해서 다운로드하기 \n",
    "    flickr = FlickrAPI(key, secret, format='parsed-json')\n",
    "    res = flickr.photos.search(\n",
    "      text = keyword,     # 키워드\n",
    "      per_page = 300,     # 검색할 개수 ( Flickr의 검색 API에서 최대 300개 이미지 제공 )\n",
    "      media = 'photos',   # 사진 검색\n",
    "      sort = \"relevance\", # 키워드 관련도 순서\n",
    "      safe_search = 1,    # 안전 검색\n",
    "      extras = 'url_q, license')\n",
    "    # 결과 확인하기\n",
    "    photos = res['photos']\n",
    "    pprint(photos)\n",
    "    try:\n",
    "      # 1장씩 다운로드하기\n",
    "      for i, photo in enumerate(photos['photo']):\n",
    "        url_q = photo['url_q']\n",
    "        filepath = savedir + '/' + photo['id'] + '.jpg'\n",
    "        if os.path.exists(filepath): continue\n",
    "        print(str(i + 1) + \":download=\", url_q)\n",
    "        urlretrieve(url_q, filepath)\n",
    "        time.sleep(wait_time)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 함수를 실행전에 디렉토리가 먼저 생성되어 있어야 한다\n",
    "\n",
    " - image\n",
    "      - sushi\n",
    "      - salad\n",
    "      - tofu\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63:download= https://live.staticflickr.com/2799/4493093951_fd34a26f30_q.jpg\n",
      "64:download= https://live.staticflickr.com/7210/6901616871_44489a0124_q.jpg\n",
      "65:download= https://live.staticflickr.com/5265/5593852399_f322052597_q.jpg\n",
      "66:download= https://live.staticflickr.com/3150/2738555364_305fdc0aa8_q.jpg\n",
      "67:download= https://live.staticflickr.com/157/364317862_7168bc5e1f_q.jpg\n",
      "68:download= https://live.staticflickr.com/2721/5736064865_c8b539852b_q.jpg\n",
      "69:download= https://live.staticflickr.com/47/142584175_760162cc6a_q.jpg\n",
      "70:download= https://live.staticflickr.com/5507/9169305943_455ce9b290_q.jpg\n",
      "71:download= https://live.staticflickr.com/7353/9599095168_764fae3cd6_q.jpg\n",
      "72:download= https://live.staticflickr.com/3061/3076416085_c1a4a08d0f_q.jpg\n",
      "73:download= https://live.staticflickr.com/6128/6189625424_5e1ea027d5_q.jpg\n",
      "74:download= https://live.staticflickr.com/2461/3909359718_6979dc2298_q.jpg\n",
      "75:download= https://live.staticflickr.com/3508/3918842460_d4265ae1bb_q.jpg\n",
      "76:download= https://live.staticflickr.com/7063/6782442058_3a35649d2c_q.jpg\n",
      "77:download= https://live.staticflickr.com/4734/39323995851_eecc870764_q.jpg\n",
      "78:download= https://live.staticflickr.com/2668/4042020925_ca2135ccda_q.jpg\n",
      "79:download= https://live.staticflickr.com/7051/6989443639_81763bde42_q.jpg\n",
      "80:download= https://live.staticflickr.com/2429/4021363868_489f288b45_q.jpg\n",
      "81:download= https://live.staticflickr.com/3920/33354510381_e7944696ef_q.jpg\n",
      "82:download= https://live.staticflickr.com/2638/4020572729_0ba507c6fe_q.jpg\n",
      "83:download= https://live.staticflickr.com/2289/2490681171_cb6b49ca00_q.jpg\n",
      "84:download= https://live.staticflickr.com/7194/6782443630_ca3061a805_q.jpg\n",
      "85:download= https://live.staticflickr.com/977/42050638502_23c2f486c1_q.jpg\n",
      "86:download= https://live.staticflickr.com/3122/3254839090_666d9342f8_q.jpg\n",
      "87:download= https://live.staticflickr.com/2593/3883965190_1ff82734c7_q.jpg\n",
      "88:download= https://live.staticflickr.com/5586/14604999840_74b0142c01_q.jpg\n",
      "89:download= https://live.staticflickr.com/4864/45770243534_dbdfd6e7c7_q.jpg\n",
      "90:download= https://live.staticflickr.com/3076/2778892153_38a329ae7b_q.jpg\n",
      "91:download= https://live.staticflickr.com/3843/15059178752_49b3e4a978_q.jpg\n",
      "92:download= https://live.staticflickr.com/3083/2779748226_6cd432f238_q.jpg\n",
      "93:download= https://live.staticflickr.com/3287/2595948649_9c82ba2225_q.jpg\n",
      "94:download= https://live.staticflickr.com/8501/8321083556_82fb4e703e_q.jpg\n",
      "95:download= https://live.staticflickr.com/3924/14615751310_7d8d76e625_q.jpg\n",
      "96:download= https://live.staticflickr.com/4323/35159195444_17e30a35b0_q.jpg\n",
      "97:download= https://live.staticflickr.com/8456/7987365371_8d1fc45bf2_q.jpg\n",
      "98:download= https://live.staticflickr.com/5563/15060206075_d308c2a9af_q.jpg\n",
      "99:download= https://live.staticflickr.com/7118/8166677899_2b6ede58c1_q.jpg\n",
      "100:download= https://live.staticflickr.com/7334/8958258251_056314225d_q.jpg\n",
      "101:download= https://live.staticflickr.com/3851/15057173931_276db17967_q.jpg\n",
      "102:download= https://live.staticflickr.com/5250/5348422111_6f74e2735c_q.jpg\n",
      "103:download= https://live.staticflickr.com/3864/15060206165_b36174ffab_q.jpg\n",
      "104:download= https://live.staticflickr.com/5130/5348410141_2336ba3142_q.jpg\n",
      "105:download= https://live.staticflickr.com/4309/35610026690_d0e59863a4_q.jpg\n",
      "106:download= https://live.staticflickr.com/4299/35828039922_a883bfc176_q.jpg\n",
      "107:download= https://live.staticflickr.com/3234/2785815174_36c7a987db_q.jpg\n",
      "108:download= https://live.staticflickr.com/3165/2627082961_9c66c6666b_q.jpg\n",
      "109:download= https://live.staticflickr.com/3440/3775320116_a0297feceb_q.jpg\n",
      "110:download= https://live.staticflickr.com/5745/22878598231_83a22cefb8_q.jpg\n",
      "111:download= https://live.staticflickr.com/8198/8166708948_fe7f04bfa3_q.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-e9a9073818a5>\", line 27, in go_download\n",
      "    time.sleep(wait_time)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 키워드와 디렉터리 이름 지정해서 다운로드하기 \n",
    "go_download('초밥', 'sushi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_download('샐러드', 'salad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_download('마파두부', 'tofu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 모두 받은 후에 관련 없는 이미지들은 삭제해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_download('햄버거','hamburger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go_download('피자','pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "go_download('육회','yukhui')\n",
    "go_download('짜장면','jjajangmyun')\n",
    "go_download('감자튀김','potato')\n",
    "go_download('탕수육','tangsuyuk')\n",
    "go_download('돈가스', 'dongas')\n",
    "go_download('물냉면', 'mul')\n",
    "go_download('잡채', 'job')\n",
    "go_download('햄버거', 'hamber')\n",
    "go_download('치킨', 'chick')\n",
    "go_download('김밥', 'kimbab')\n",
    "go_download('라면', 'ramy')\n",
    "go_download('족발', 'jok')\n",
    "go_download('두부김치', 'dubokim')\n",
    "go_download('순대', 'sundae')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selenium으로 URL 접속하기\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "driver = webdriver.Chrome(\"D:/MyClass/Python/fML/09_photo_calorie_class/webdriver/chromedriver.exe\")   \n",
    "\n",
    "\n",
    "# result = []\n",
    "# order = []\n",
    "# menulist=['미스터피자']\n",
    "\n",
    "# for m in menulist:\n",
    "#     driver.get(f\"https://web.dominos.co.kr/goods/list?dsp_ctgr=C0101\")\n",
    "#     time.sleep(3)\n",
    "#     #product_content_name_0 > img\n",
    "#     for i in range(0,18):\n",
    "\n",
    "#         image_path = driver.find_element_by_xpath('//*[@id=\"product_content_name_'+str(i)+'\"]/img')\n",
    "#                                                     #//*[@id=\"product_content_name_18\"]/img\n",
    "#         src = image_path.get_attribute('src')\n",
    "#         print(src)\n",
    "#         urllib.request.urlretrieve(src, \"./피자/\"+m+str(i)+\".png\")\n",
    "    \n",
    "#           for i, photo in enumerate(photos['photo']):\n",
    "#         url_q = photo['url_q']\n",
    "#         filepath = savedir + '/' + photo['id'] + '.jpg'\n",
    "#         if os.path.exists(filepath): continue\n",
    "#         print(str(i + 1) + \":download=\", url_q)\n",
    "#         urlretrieve(url_q, filepath)\n",
    "#         time.sleep(wait_time)\n",
    "\n",
    "#     ret = pd.DataFrame(result)\n",
    "#     ret.columns = ['가게명', '중분류', '주소','전화번호']\n",
    "#     ret.to_csv(\"./\"+m+\".csv\")\n",
    "#     ordm = pd.DataFrame(order)\n",
    "#     ordm.to_csv(\"./\"+m+\"메뉴.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium\n",
    "#pip install bs4\n",
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(\"D:/MyClass/Python/fML/09_photo_calorie_class/webdriver/chromedriver.exe\")  \n",
    "\n",
    "#웹페이지의 소스를 가져온다.\n",
    "driver.get(f\"https://web.dominos.co.kr/goods/list?dsp_ctgr=C0101\")\n",
    "time.sleep(3)\n",
    "html = driver.page_source\n",
    "#소스에서 img_area 클래스 하위의 소스를 가져온다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img = soup.find_all(class_='lazyload')\n",
    "m='도미노피자'\n",
    "for i,j in enumerate(img):\n",
    "    src = j[\"data-src\"]\n",
    "    print(src)\n",
    "    print(i)\n",
    "    urllib.request.urlretrieve(src, \"./피자/\"+m+str(i)+\".jpg\")\n",
    "\n",
    "#이미지 경로를 받아온다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "#웹페이지의 소스를 가져온다.\n",
    "\n",
    "driver.get(f\"http://pizzaschool.net/menu/\")\n",
    "time.sleep(3)\n",
    "html = driver.page_source\n",
    "#소스에서 img_area 클래스 하위의 소스를 가져온다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img = soup.find_all(class_='external-img wp-post-image')\n",
    "m='피자스쿨'\n",
    "print(img)\n",
    "\n",
    "# for i,j in enumerate(img):\n",
    "#     src = j[\"src\"]\n",
    "#     print(src)\n",
    "#     print(i)\n",
    "#     urllib.request.urlretrieve(src, \"./피자/\"+m+str(i)+\".jpg\")\n",
    "# #이미지 경로를 받아온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "#웹페이지의 소스를 가져온다.\n",
    "\n",
    "driver.get(f\"http://www.krispykreme.co.kr/menu/doughnut.asp\")\n",
    "time.sleep(3)\n",
    "html = driver.page_source\n",
    "#소스에서 img_area 클래스 하위의 소스를 가져온다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img = soup.find_all(class_='cate_img')\n",
    "m='크리스피도넛'\n",
    "for i,j in enumerate(img):\n",
    "    src = j[\"src\"]\n",
    "    src = 'http://www.krispykreme.co.kr/'+src\n",
    "    print(i)\n",
    "    urllib.request.urlretrieve(src, \"./도넛/\"+m+str(i)+\".jpg\")\n",
    "\n",
    "#이미지 경로를 받아온다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "#웹페이지의 소스를 가져온다.\n",
    "\n",
    "driver.get(f\"http://www.dunkindonuts.co.kr/menu/main.php?top=A\")\n",
    "time.sleep(3)\n",
    "html = driver.page_source\n",
    "#소스에서 img_area 클래스 하위의 소스를 가져온다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img = soup.select('.product > figure > img')\n",
    "m='던킨도넛'\n",
    "\n",
    "m='도미노피자'\n",
    "for i,j in enumerate(img):\n",
    "    src = j[\"src\"]\n",
    "    src = 'http://www.dunkindonuts.co.kr/'+src\n",
    "    print(src)\n",
    "    print(i)\n",
    "    urllib.request.urlretrieve(src, \"./도넛/\"+m+str(i)+\".jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_download('도넛', 'donut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "\n",
    "#웹페이지의 소스를 가져온다.\n",
    "\n",
    "driver.get(f\"https://www.burgerking.co.kr/#/menu/K100004/%EC%A3%BC%EB%8B%88%EC%96%B4&%EB%B2%84%EA%B1%B0\")\n",
    "time.sleep(3)\n",
    "html = driver.page_source\n",
    "#소스에서 img_area 클래스 하위의 소스를 가져온다.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img = soup.select('.prd_img > span > img')\n",
    "m='버거킹햄버거주니어와퍼'\n",
    "\n",
    "for i,j in enumerate(img):\n",
    "    src = j[\"src\"]\n",
    "    src = src\n",
    "    print(src)\n",
    "    print(i)\n",
    "    urllib.request.urlretrieve(src, \"./햄버거/\"+m+str(i)+\".jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
