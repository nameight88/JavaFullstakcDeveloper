{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "+ mnist : 머신러닝의 고전적인 문제로 숫자를 손으로 쓴 글자를 모아놓은 데이터세트\n",
    "+ fashion mnist : 옷, 신발, 가방등의 28*28 픽셀의 이미지 데이타세트\n",
    "\n",
    "                            [ Fashion MNIST 의 범주 ]\n",
    "    <table>\n",
    "        <tr><th>라벨</th><th>범주</th></tr>\n",
    "    <tr><td>0</td><td>티셔츠(상의)</td></tr>\n",
    "    <tr><td>1</td><td>바지</td></tr>\n",
    "    <tr><td>2</td><td>스웨터</td></tr>\n",
    "    <tr><td>3</td><td>드레스</td></tr>\n",
    "    <tr><td>4</td><td>코트</td></tr>\n",
    "    <tr><td>5</td><td>샌들</td></tr>\n",
    "    <tr><td>6</td><td>셔츠</td></tr>\n",
    "    <tr><td>7</td><td>운동화</td></tr>\n",
    "    <tr><td>8</td><td>가방</td></tr>\n",
    "    <tr><td>9</td><td>부츠</td></tr>\n",
    "    </table>\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# (1) Fashion MNIST 데이터셋 불러오기\n",
    "\n",
    "import tensorflow as tf\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_X, train_Y), (test_X, test_Y) = fashion_mnist.load_data()\n",
    "\n",
    "# 데이타 수 확인\n",
    "print(len(train_X), len(test_X))\n",
    "\n",
    "# 데이타 형태 확인\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# (2) 데이터 확인\n",
    "#      imshow() : 이미지를 그래프 형태로 표시\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_X[0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(train_Y[0])  # 정답(라벨)을 확인 ( 9 : 부츠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
      "  0.         0.00392157 0.01568627 0.         0.         0.\n",
      "  0.         0.00392157 0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
      "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
      "  0.01568627 0.         0.         0.01176471]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
      "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
      "  0.         0.04705882 0.03921569 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
      "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
      "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.00392157\n",
      "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
      "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
      "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
      "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
      "  0.48235294 0.76862745 0.89803922 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
      "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
      "  0.8745098  0.96078431 0.67843137 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
      "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
      "  0.8627451  0.95294118 0.79215686 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
      "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
      "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02352941 0.\n",
      "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
      "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
      "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01568627 0.         0.\n",
      "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
      "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
      "  0.85098039 0.81960784 0.36078431 0.        ]\n",
      " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
      "  0.00784314 0.         0.         0.         0.         0.\n",
      "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
      "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
      "  0.85490196 1.         0.30196078 0.        ]\n",
      " [0.         0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
      "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
      "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
      "  0.87843137 0.95686275 0.62352941 0.        ]\n",
      " [0.         0.         0.         0.         0.07058824 0.17254902\n",
      "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
      "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
      "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
      "  0.91372549 0.93333333 0.84313725 0.        ]\n",
      " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
      "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
      "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
      "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
      "  0.8627451  0.90980392 0.96470588 0.        ]\n",
      " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
      "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
      "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
      "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
      "  0.87058824 0.89411765 0.88235294 0.        ]\n",
      " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
      "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
      "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
      "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
      "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
      " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
      "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
      "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
      "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
      "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
      " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
      "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
      "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
      "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
      "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
      " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
      "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
      "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
      "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
      "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
      " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
      "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
      "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
      "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
      "  0.75294118 0.84705882 0.66666667 0.        ]\n",
      " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
      "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
      "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
      "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
      "  0.38823529 0.22745098 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
      "  0.1372549  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# (3) 데이터 정규화\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0\n",
    "\n",
    "print(train_X[0])\n",
    "# [결과] 모든 데이타가 0~1 사이의 값을 되도록 정규화가 잘 되었음을 확인\n",
    "\n",
    "# 앞에서 to_categorical() 를 이용하여 one-hot encoding 작업을 했었는데 \n",
    "# 10개의 답을 위해 [1 0 0 0 0 0 0 0 0 0] 으로 계산하는 것이 오히려 더 시스템 낭비일 수가 있다.\n",
    "# 그래서 희소행렬을 이용하는데 categorical_crossentropy 손실율 계산시 sparse_categorical_crossentropy를 사용하여\n",
    "# 데이타 전처리 없이 희소 행렬을 나타내는 데이타를 정답 행렬로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "\n",
    "   + 이차원 배열을 일차원 배열로 바꿔주는 함수\n",
    "   + 컨볼루션 층(Convolution layer)나 맥스플링(MaxPooling)은 주어진 이미지를 2차원 배열인 채로 다룬다.\n",
    "     그 뒤 일반층으로 넘길 때 Flatten() 함수를 이용한다\n",
    "\n",
    "\n",
    "## 활성화 함수 (Activation Function)\n",
    "\n",
    "   + 입력을 비선형 출력으로 변환해주는 함수\n",
    "   + 자주 사용되는 활성화 함수\n",
    "       - 시그모이드 (sigmoid)\n",
    "       - 하이퍼볼릭 탄젠트 (tanh)\n",
    "       - ReLU\n",
    "       \n",
    "       [공식문서] https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "\n",
    "       \n",
    "## 입력과 출력\n",
    "\n",
    "   + 첫번째 레이어는 입력 데이터 형태로 이미지의 가로, 세로 배열값으로 input_shape=(28,28)\n",
    "   + 마지막 레이어는 출력층으로 반드시 분류해야하는 클래스 수와 동일해야 함\n",
    "       - mnist는 0~9까지 총 10개의 분류값(클래스)로 되어 있기에 출력층 노드는 10이여야 함\n",
    "       - 출력층의 노드가 1개인 경우는 sigmoid 활성화 함수를 적용하지만\n",
    "           - tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "       - 출력층의 노드가 2개 이상인 경우는 softmax 활성화 함수를 적용한다.\n",
    "           - tf.keras.layers.Dense(units=2, activation='softmax')\n",
    "           \n",
    "           \n",
    "## 손실함수        \n",
    "\n",
    "   + 모델의 출력층에 따라 손실함수를 설정해야 정상적인 훈련이 가능\n",
    "   + compile() 함수에 loss지정\n",
    "    \n",
    "    \n",
    "   + 이진분류 : 출력층의 노드 = 1, sigmoid 활성화 함수\n",
    "        - model.compile(loss='binary_crossentropy')\n",
    "        \n",
    "        \n",
    "   + 출력층의 노드 = 2개 이상, softmax 활성화 함수\n",
    "        - model.compile(loss='categorical_crossentropy' ) : 출력 데이타(y)가 one-hot vector인 경우\n",
    "        - model.compile(loss='sparse_categorical_crossentropy' ) : 출력 데이타(y)가 one-hot vector가 아닌 경우\n",
    "        - mnist 데이타는 클래스의 수가 10개로 원핫벡터가 아니라 0~9까지 레이블 값을 가진다\n",
    "        \n",
    "         [참고] one-hot vector : [0,0,1,0,0,0,0,0,0,0]\n",
    "     \n",
    "\n",
    "## 옵티마이저 (optimizer )\n",
    "\n",
    "   + 손실을 낮추기 위해 신경망의 가중치 같은 속성 변경하는데 사용되는 최적화 방법\n",
    "   + 일반적으로 많이 사용하는 알고리즘 : Adam\n",
    "   + 케라스에서 지원하는 옵티마이저 :\n",
    "       -   SGD, Adam, Adgrad, Nadam, RMRprop, Adadelta, Adamax, Ftrl   \n",
    "       \n",
    "          [공식문서] https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "   \n",
    "   \n",
    "      + 클래스 인스턴스로 지정 \n",
    "           - adam = tf.keras.optimizers.Adam(lr=0.001)\n",
    "             model.compile(optimizer=adam)\n",
    "           - lr (Learning rate:학습률) 같은 하이퍼파라미터르 직접 지정 가능\n",
    "           \n",
    "      + 문자열 지정 (전부 소문자임 )\n",
    "           - model.compile(optimizer='adam')\n",
    "           \n",
    "           \n",
    "## 평가지표 (metrics)\n",
    "\n",
    "   + 분류모델에 대한 평가지표는 정확도를 나탸내는 'accurary' 또는 'acc'가 많이 사용\n",
    "   + 'auc', 'precision', 'recall' 등의 지표도 사용\n",
    "   \n",
    "       [공식문서] https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 1. 모델 설정하기 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 2. 학습하기(훈련) </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.5222 - accuracy: 0.8184 - val_loss: 0.4212 - val_accuracy: 0.8524\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.3860 - accuracy: 0.8608 - val_loss: 0.3880 - val_accuracy: 0.8603\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.3465 - accuracy: 0.8732 - val_loss: 0.3575 - val_accuracy: 0.8711\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.3215 - accuracy: 0.8825 - val_loss: 0.3346 - val_accuracy: 0.8797\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.3002 - accuracy: 0.8903 - val_loss: 0.3902 - val_accuracy: 0.8686\n",
      "Epoch 6/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.2850 - accuracy: 0.8949 - val_loss: 0.3325 - val_accuracy: 0.8797\n",
      "Epoch 7/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.2738 - accuracy: 0.8982 - val_loss: 0.3185 - val_accuracy: 0.8859\n",
      "Epoch 8/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.2612 - accuracy: 0.9024 - val_loss: 0.3297 - val_accuracy: 0.8829\n",
      "Epoch 9/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2509 - accuracy: 0.9064 - val_loss: 0.3300 - val_accuracy: 0.8849\n",
      "Epoch 10/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2434 - accuracy: 0.9092 - val_loss: 0.3218 - val_accuracy: 0.8891\n",
      "Epoch 11/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2342 - accuracy: 0.9127 - val_loss: 0.3187 - val_accuracy: 0.8855\n",
      "Epoch 12/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.2262 - accuracy: 0.9153 - val_loss: 0.3143 - val_accuracy: 0.8902\n",
      "Epoch 13/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2174 - accuracy: 0.9190 - val_loss: 0.3495 - val_accuracy: 0.8776\n",
      "Epoch 14/25\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.2117 - accuracy: 0.9214 - val_loss: 0.3275 - val_accuracy: 0.8885\n",
      "Epoch 15/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9242 - val_loss: 0.3358 - val_accuracy: 0.8873\n",
      "Epoch 16/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.1988 - accuracy: 0.9252 - val_loss: 0.3514 - val_accuracy: 0.8843\n",
      "Epoch 17/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.1930 - accuracy: 0.9288 - val_loss: 0.3480 - val_accuracy: 0.8851\n",
      "Epoch 18/25\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.1870 - accuracy: 0.9303 - val_loss: 0.3317 - val_accuracy: 0.8909\n",
      "Epoch 19/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9330 - val_loss: 0.3407 - val_accuracy: 0.8877\n",
      "Epoch 20/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9324 - val_loss: 0.3532 - val_accuracy: 0.8883\n",
      "Epoch 21/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1712 - accuracy: 0.9362 - val_loss: 0.3387 - val_accuracy: 0.8939\n",
      "Epoch 22/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1659 - accuracy: 0.9385 - val_loss: 0.3306 - val_accuracy: 0.8946\n",
      "Epoch 23/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1607 - accuracy: 0.9408 - val_loss: 0.3923 - val_accuracy: 0.8802\n",
      "Epoch 24/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1592 - accuracy: 0.9410 - val_loss: 0.3719 - val_accuracy: 0.8888\n",
      "Epoch 25/25\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.1559 - accuracy: 0.9415 - val_loss: 0.3697 - val_accuracy: 0.8879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> 3. 평가하기 </font>\n",
    "\n",
    "\n",
    "   + 모델 성능을 검증하고 평가\n",
    "   + 컴파일 단계에서 지정한 손실과 정확도를 순서대로 반환하여 출력\n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 912us/step - loss: 0.4002 - accuracy: 0.8803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4002304673194885, 0.880299985408783]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
