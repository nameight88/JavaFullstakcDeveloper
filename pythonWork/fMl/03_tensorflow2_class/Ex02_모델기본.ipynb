{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## [비교] 사이킷런 모델과 케라스 모델\n",
    "\n",
    "#### [ 사이킷런]\n",
    "    1- 모델    sc = SGDClassifier( loss='log', max_iter=5 )\n",
    "                                log : 손실함수, 반복횟수 5번\n",
    "    2- 훈련    sc.fit( train_scaled, train_target )\n",
    "    3- 평가    sc.score( test_scaled, test_target ) \n",
    "    \n",
    "#### [ 케라스 ]  \n",
    "               dense = keras.layers.Dense( 10, activation='softmax', input_shape=(784,))\n",
    "    1- 모델    model = keras.Sequential(dense)\n",
    "               model.compile( loss='sparse_categorial_crossentropy', metrics='accuracy')\n",
    "                                 sparse_categorial_crossentropy : 손실함수\n",
    "    2- 훈련    model.fit( train_scaled, train_target, epochs=5 )\n",
    "                                                      반복횟수 : 5\n",
    "    3- 평가    model.evaluate( test_scaled, test_target )\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  [  모델 구동  절차 ]\n",
    "\n",
    "0. 데이타 전처리\n",
    "\n",
    "1. 모델 생성\n",
    "   - model = Sequential() \n",
    "   - 딥러닝의 구조를 짜고 층을 설정하는 부분입니다. \n",
    "\n",
    "2. 모델 컴파일\n",
    "     - model.compile() \n",
    "     - 위에서 정해진 모델을 컴퓨터가 알아들을 수 있게끔 컴파일 하는 부분입니다. \n",
    "\n",
    "3. 모델 훈련\n",
    "    - model.fit()\n",
    "    - 모델을 실제로 수행하는 부분입니다. \n",
    "    \n",
    "4. 모델 검증\n",
    "\n",
    "5. 모델 예측\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) 데이타처리\n",
    "\n",
    "+ 주어진 폐암 수술 환자의 생존 여부 데이터는 총 470명의 환자에게서 17개의 정보를 정리한 것입니다.\n",
    "+ 각 정보를 ‘속성’이라고 부릅니다. 그리고 생존 여부를 클래스, 가로 한 줄에 해당하는 각 환자의 정보를 각각 ‘샘플’이라고 합니다. \n",
    "+ 주어진 데이터에는 총 470개의 샘플이 각각 17개씩의 속성을 가지고 있는 것입니다.\n",
    "\n",
    "<img src='./imgs/model3.PNG' width='500'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[293.     1.     3.8  ...   1.     0.    62.  ]\n",
      " [  1.     2.     2.88 ...   1.     0.    60.  ]\n",
      " [  8.     2.     3.19 ...   1.     0.    66.  ]\n",
      " ...\n",
      " [406.     6.     5.36 ...   0.     0.    62.  ]\n",
      " [ 25.     8.     4.32 ...   0.     0.    58.  ]\n",
      " [447.     8.     5.2  ...   0.     0.    49.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "  \n",
    "# 준비된 수술 환자 데이터를 불러옴\n",
    "data_set = np.loadtxt(\"./dataset/ThoraricSurgery.csv\", delimiter=\",\")\n",
    "  \n",
    "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장\n",
    "X = data_set[:,0:17]\n",
    "Y = data_set[:,17]\n",
    "print(X)\n",
    "#print(X[0])   # 17 개 항목 \n",
    "#print(Y[0])   # 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42.    2.    3.24  2.52  1.    0.    0.    0.    1.    0.   12.    0.\n",
      "  0.    0.    1.    0.   63.  ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#생존한 사람의 정보\n",
    "print(X[7])\n",
    "print(Y[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">540</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │             \u001b[38;5;34m540\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m31\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">571</span> (2.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m571\u001b[0m (2.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">571</span> (2.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m571\u001b[0m (2.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# [1]  모델 생성 - 리스트형\n",
    "model = tf.keras.Sequential(\n",
    "[\n",
    "    tf.keras.layers.Dense(units = 30, activation=\"relu\",input_dim=17),\n",
    "    #유낫을 30개를 주고 활성화 하는 공식을 relu를 이용을 하고 데이터 종류가 17개이다.\n",
    "    tf.keras.layers.Dense(units = 1, activation=\"sigmoid\")\n",
    "    \n",
    "]\n",
    ")\n",
    "\n",
    "# [2]  모델 생성 - add 함수\n",
    "\n",
    "#model = tf.keras.Sequential()\n",
    "#model.add\n",
    "\n",
    "# [참고] 모델 요약\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./imgs/model1.PNG' width='400'/>\n",
    "\n",
    "- 이제 Dense(30, input_dim=17, activation='relu') 부분을 더 살펴보겠습니다. \n",
    "- 30이라고 되어 있는 것은 이 층에 30개의 노드를 만들겠다는 것입니다. \n",
    "- input_dim이라는 변수가 나옵니다. 이는 입력 데이터에서 몇 개의 값을 가져올지를 정하는 것입니다. \n",
    "- keras는 입력층을 따로 만드는 것이 아니라, 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸합니다. \n",
    "- 우리가 다루고 있는 폐암 수술 환자의 생존 여부 데이터에는 17개의 입력 값들이 있습니다. 따라서 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다는 뜻입니다.\n",
    "\n",
    "\n",
    "\n",
    "    + 17개 입력데이타가 30개의 노드로 들어가니 17 * 30 이고 30 개에서 다시 하나의 출력층으로 가니 30을 더해\n",
    "        17 * 30 + 30 -> 540\n",
    "    + 30개의 노드에서 하나의 출력층으로 들어오고 하나의 출력층에서 나오니 30 + 1 -> 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (2) 모델 컴파일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### - model.compile 부분은 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러 가지 환경을 설정해 주면서 컴파일하는 부분입니다. \n",
    "##### - loss : 한 번 신경망이 실행될 때마다 오차 값을 추적하는 함수\n",
    "##### - optimizer : 오차를 어떻게 줄여 나갈지 정하는 함수\n",
    "\n",
    "\n",
    "   - 옵티마이저 : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "   - 손실함수 :  https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "   - 평가함수 :  https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "\n",
    "\n",
    "\n",
    "- 먼저 어떤 오차 함수를 사용할지를 정해야 합니다. 여기서는 평균 제곱 오차 함수(mean_squared_error)를 사용했습니다.\n",
    "- 그런데 경우에 따라서는 오차 함수를 바꾸면 더 좋은 효과를 나타내기도 합니다. 오차 함수에는 평균 제곱 오차 계열의 함수 외에도 교차 엔트로피 계열의 함수가 있습니다\n",
    "- 교차 엔트로피는 주로 분류 문제에서 많이 사용되는데, 특별히 예측 값이 참과 거짓 둘 중 하나인 형식일 때는 binary_crossentropy(이항 교차 엔트로피)를 씁니다. 지금 구하고자 하는 것은 생존(1) 또는 사망(0) 둘 중 하나이므로 binary_crossentropy를 사용할 수 있는 좋은 예라고 할 수 있습니다.\n",
    "\n",
    "#### 교차엔트로피\n",
    "<img src='./imgs/model2.PNG' width='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (3) 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 624us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 2/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 3/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 4/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 5/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 6/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 7/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 8/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 9/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 10/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 11/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 12/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 13/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 14/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 15/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 16/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 17/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 18/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 19/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 20/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 21/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 22/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 23/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 24/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 25/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 26/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 27/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 28/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 29/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 30/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 31/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 32/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 33/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 34/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 35/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 36/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 37/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 38/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 39/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 40/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 562us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 41/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 42/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 43/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 44/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 45/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 46/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 47/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 48/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 49/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 50/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 51/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 52/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 53/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 54/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 55/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 56/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 57/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 58/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 59/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 60/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 61/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 62/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 63/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 64/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 65/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 587us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 66/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 68/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 69/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 70/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 71/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 72/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 610us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 73/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 74/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 75/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 76/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 77/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 78/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 79/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 80/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 81/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 82/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 83/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 84/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 85/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 86/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 87/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 88/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 89/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 90/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 91/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 92/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 93/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 94/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 95/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 96/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 97/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 98/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 99/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - accuracy: 0.8583 - loss: 0.1417\n",
      "Epoch 100/100\n",
      "\u001b[1m470/470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - accuracy: 0.8583 - loss: 0.1417\n"
     ]
    }
   ],
   "source": [
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분\n",
    "\n",
    "history=model.fit(X,Y,epochs=100,batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 주어진 폐암 수술 환자의 생존 여부 데이터는 총 470명의 환자에게서 17개의 정보를 정리한 것입니다.\n",
    "+ 각 정보를 ‘속성’이라고 부릅니다. 그리고 생존 여부를 클래스, 가로 한 줄에 해당하는 각 환자의 정보를 각각 ‘샘플’이라고 합니다. \n",
    "+ 주어진 데이터에는 총 470개의 샘플이 각각 17개씩의 속성을 가지고 있는 것입니다.\n",
    "\n",
    "<img src='./imgs/model3.PNG' width='500'/>\n",
    "\n",
    "\n",
    "+ 학습 프로세스가 모든 샘플에 대해 한 번 실행되는 것을 1 epoch(‘에포크’라고 읽습니다)라고 합니다. \n",
    "+ 코드에서 epochs=100으로 지정한 것은 각 샘플이 처음부터 끝까지 100번 재사용될 때까지 실행을 반복하라는 뜻입니다.\n",
    "\n",
    "+ batch_size는 샘플을 한 번에 몇 개씩 처리할지를 정하는 부분으로 batch_size=10은 전체 470개의 샘플을 10개씩 끊어서 집어넣으라는 뜻이 됩니다. \n",
    "+ batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결괏값이 불안정해질 수 있습니다. 따라서 자신의 컴퓨터 메모리가 감당할 만큼의 batch_size를 찾아 설정해 주는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (4) 결과\n",
    "\n",
    "+ 정확도(accuracy) 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.8366 - loss: 0.1634\n",
      "test_loss:  0.1489361673593521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8510638475418091"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과를 출력\n",
    "\n",
    "test_loss,test_accuracy=model.evaluate(X,Y)\n",
    "\n",
    "print(\"test_loss: \",test_loss)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (5) 예측\n",
    "\n",
    "+ 마지막 데이타로 확인\n",
    "\n",
    "447\t8\t5.2\t4.1\t0\t0\t0\t0\t0\t0\t12\t0\t0\t0\t0\t0\t49               0 (사망)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0619967e-14]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7번째 생존한 사람의 정보를 넣고 결과를 예측 했을때\n",
    "pre_x = [[42.  ,  2.  ,  3.24,  2.52 , 1. ,   0. ,   0.  ,  0.  ,  1. ,   0. ,  12.  ,  0.,\n",
    "  0.   , 0.  ,  1.  ,  0.  , 63.  ]]\n",
    "\n",
    "pre_x= np.array(pre_x)\n",
    "\n",
    "model.predict(pre_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.109643e-16]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3번째 생존한 사람의 정보를 넣고 난 후에 결과 예측\n",
    "pre_x = [[14.    ,2. ,   3.98 , 3.06 , 2.   , 0.,   0. ,   0. ,   1. ,   1.  , 14.,   0.,\n",
    "  0.  ,  0.  ,  1.  ,  0.  , 80.  ]]\n",
    "\n",
    "pre_x= np.array(pre_x)\n",
    "\n",
    "model.predict(pre_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측결과\n",
    "\n",
    "0에 가까움"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
